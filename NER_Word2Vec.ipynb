{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER Word2Vec",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vincentjordan27/Named-Entity-Recognition-BILSTM-CRF/blob/main/NER_Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1Mc6xFkx2gU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbd85237-df7e-4d9b-b805-4de97d293655"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4z2HSMQyvBa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f7bdd21-faf0-4c38-c6fa-2c4c3d371624"
      },
      "source": [
        "!pip install torchtext==0.6.0\n",
        "\n",
        "import os\n",
        "import time\n",
        "import gensim\n",
        "from collections import Counter\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torchtext.data import Field, BucketIterator\n",
        "from torchtext.datasets import SequenceTaggingDataset\n",
        "from torchtext.vocab import Vocab\n",
        "from spacy.lang.id import Indonesian\n",
        "\n",
        "DRIVE_ROOT = \"/content/gdrive/My Drive/Dataset/Input\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext==0.6.0 in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (0.1.96)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (4.62.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.6.0) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPVOphrU2lb1"
      },
      "source": [
        "## Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbE_8iN0J_m0"
      },
      "source": [
        "Untuk corpus preperation kami menggunakan torchtext dan word embedding dengan Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apRCjTMZx7O5"
      },
      "source": [
        "class Corpus(object):\n",
        "\n",
        "    def __init__(self, input_folder, min_word_freq, batch_size, wv_file=None):\n",
        "        # list all the fields\n",
        "        self.word_field = Field(lower=True)\n",
        "        self.tag_field = Field(unk_token=None)\n",
        "        # create dataset using built-in parser from torchtext\n",
        "        self.train_dataset, self.val_dataset, self.test_dataset = SequenceTaggingDataset.splits(\n",
        "            path=input_folder,\n",
        "            train=\"train.tsv\",\n",
        "            validation=\"val.tsv\",\n",
        "            test=\"test.tsv\",\n",
        "            fields=((\"word\", self.word_field), (\"tag\", self.tag_field))\n",
        "        )\n",
        "        ### BEGIN MODIFIED SECTION: WORD EMBEDDING ###\n",
        "        if wv_file:\n",
        "            # retrieve word2vec model from gensim library\n",
        "            # the file contains full word2vec model, not only key-vectors\n",
        "            self.wv_model = gensim.models.word2vec.Word2Vec.load(wv_file)\n",
        "            self.embedding_dim = self.wv_model.vector_size\n",
        "            # cannot create vocab with build_vocab(),\n",
        "            # initiate vocab by building custom Counter based on word2vec model\n",
        "            word_freq = {word: self.wv_model.wv.vocab[word].count for word in self.wv_model.wv.vocab}\n",
        "            word_counter = Counter(word_freq)\n",
        "            self.word_field.vocab = Vocab(word_counter, min_freq=min_word_freq)\n",
        "            # mapping each vector/embedding from word2vec model to word_field vocabs\n",
        "            vectors = []\n",
        "            for word, idx in self.word_field.vocab.stoi.items():\n",
        "                if word in self.wv_model.wv.vocab.keys():\n",
        "                    vectors.append(torch.as_tensor(self.wv_model.wv[word].tolist()))\n",
        "                else:\n",
        "                    vectors.append(torch.zeros(self.embedding_dim))\n",
        "            self.word_field.vocab.set_vectors(\n",
        "                stoi=self.word_field.vocab.stoi,\n",
        "                # list of vector embedding, orderred according to word_field.vocab\n",
        "                vectors=vectors,\n",
        "                dim=self.embedding_dim\n",
        "            )\n",
        "        else:\n",
        "            self.word_field.build_vocab(self.train_dataset.word, min_freq=min_word_freq)\n",
        "        ### END MODIFIED SECTION ###\n",
        "        # build vocab for tag\n",
        "        self.tag_field.build_vocab(self.train_dataset.tag)\n",
        "        # create iterator for batch input\n",
        "        self.train_iter, self.val_iter, self.test_iter = BucketIterator.splits(\n",
        "            datasets=(self.train_dataset, self.val_dataset, self.test_dataset),\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "        # prepare padding index to be ignored during model training/evaluation\n",
        "        self.word_pad_idx = self.word_field.vocab.stoi[self.word_field.pad_token]\n",
        "        self.tag_pad_idx = self.tag_field.vocab.stoi[self.tag_field.pad_token]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHIQI37F0AuZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61d77e8a-f2f6-429f-a74e-824f43b0b39c"
      },
      "source": [
        "corpus = Corpus(\n",
        "    input_folder=f\"{DRIVE_ROOT}/\",\n",
        "    min_word_freq=3,\n",
        "    batch_size=64,\n",
        "    wv_file=f\"{DRIVE_ROOT}/embeddings/id_ft.bin\"\n",
        ")\n",
        "print(f\"Train set: {len(corpus.train_dataset)} sentences\")\n",
        "print(f\"Val set: {len(corpus.val_dataset)} sentences\")\n",
        "print(f\"Test set: {len(corpus.test_dataset)} sentences\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set: 3535 sentences\n",
            "Val set: 470 sentences\n",
            "Test set: 468 sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx7faLRH-dgh"
      },
      "source": [
        "Let's take a look at the loaded word2vec model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdFBR4g0-iX_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95310232-a955-44b6-f497-243d42d0d40b"
      },
      "source": [
        "wv_shape = corpus.wv_model.wv.vectors.shape\n",
        "print(f\"The model was trained on {corpus.wv_model.corpus_count:,} words.\")\n",
        "print(f\"The embedding represents {wv_shape[0]:,} unique words with vectors of size {wv_shape[1]}.\")\n",
        "print(\"The 5 most similar words to 'pemberhentian': \" + \", \".join([word for word, prob in corpus.wv_model.wv.most_similar(\"pemberhentian\",topn=5)]) + \".\")\n",
        "print(\"An example of embeddings for the word 'pemberhentian' (which does not exist in the training set):\")\n",
        "print(corpus.wv_model.wv[\"pemberhentian\"])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model was trained on 2,988,507 words.\n",
            "The embedding represents 30,048 unique words with vectors of size 300.\n",
            "The 5 most similar words to 'pemberhentian': pengangkatan, pemecatan, penunjukkan, memberhentikan, diberhentikan.\n",
            "An example of embeddings for the word 'pemberhentian' (which does not exist in the training set):\n",
            "[ 8.51707399e-01 -4.16559912e-02  6.00995362e-01  5.37340283e-01\n",
            "  9.33561563e-01 -4.98508543e-01  1.66677666e+00 -4.50234622e-01\n",
            " -1.04996562e+00 -1.16242014e-01 -9.65069592e-01 -5.75939476e-01\n",
            "  1.31526738e-01 -9.09028769e-01  1.10143505e-01 -9.74609315e-01\n",
            " -5.14446139e-01 -7.96002030e-01  9.62427914e-01 -1.68779945e+00\n",
            " -2.55186737e-01  1.85162282e+00 -1.72700420e-01 -5.72440922e-01\n",
            "  2.40831465e-01  3.36628050e-01 -2.71441191e-01 -1.88790828e-01\n",
            "  1.02529240e+00  3.33210170e-01  2.38483861e-01 -5.34359038e-01\n",
            "  2.23073304e-01 -1.43152475e+00  9.19080257e-01  6.59672499e-01\n",
            " -6.07612550e-01 -2.71360338e-01  1.97121471e-01 -1.79148242e-01\n",
            "  2.39485770e-01  1.60026953e-01  4.91577983e-01  8.12102556e-01\n",
            " -3.14166874e-01 -2.11117005e+00 -2.62773007e-01 -3.65308911e-01\n",
            "  1.68337440e+00  4.35764611e-01 -6.20461464e-01 -2.90079236e-01\n",
            "  1.08306921e+00  9.72508013e-01  1.18159187e+00  1.64748049e+00\n",
            " -9.04088140e-01 -1.09151369e-02 -3.18961978e-01  4.47677284e-01\n",
            " -2.10084915e-01  4.01785284e-01 -5.19243956e-01 -4.25561428e-01\n",
            "  9.02218744e-02  5.22421300e-01  3.30280542e-01 -6.96946919e-01\n",
            "  2.03404069e-01  4.41009015e-01 -5.16085505e-01 -5.63966393e-01\n",
            "  2.65587509e-01  1.17342424e+00  1.41310644e+00  2.36623496e-01\n",
            " -3.02156925e-01 -2.10411608e-01 -3.49590808e-01 -3.62917274e-01\n",
            " -3.97976965e-01 -8.11088443e-01 -1.30235016e-01 -2.74186842e-02\n",
            "  1.52894557e-01 -5.78244686e-01 -6.87207699e-01  3.71764034e-01\n",
            " -1.02130873e-02 -1.03774481e-01  9.18471396e-01  3.77998143e-01\n",
            "  8.29615533e-01 -4.64594036e-01 -1.95853293e+00  8.41598988e-01\n",
            " -6.97278082e-01 -1.39127982e+00 -1.17970908e+00  6.26435876e-01\n",
            "  4.36796367e-01  2.40126938e-01  3.46666664e-01 -5.50140917e-01\n",
            "  1.29031074e+00  1.81465194e-01  1.29474175e+00 -2.94572324e-01\n",
            " -1.91097766e-01  5.79109967e-01  5.56693316e-01  5.15562057e-01\n",
            " -9.67895985e-01 -4.51913565e-01 -5.98575622e-02  1.51058614e-01\n",
            " -3.04100752e-01  6.43869698e-01 -7.85730124e-01 -3.61271910e-02\n",
            "  2.59683758e-01 -3.03502351e-01  2.99557269e-01 -2.33917966e-01\n",
            "  6.56410828e-02 -8.29924643e-01 -3.86961132e-01 -7.04598784e-01\n",
            " -2.22051404e-02  6.99441731e-01 -9.86097157e-01 -1.13566959e+00\n",
            " -6.90665662e-01  9.49239507e-02 -1.69790542e+00  2.89977163e-01\n",
            "  1.44660664e+00  1.61305463e+00 -4.33293968e-01 -9.40357864e-01\n",
            "  4.68411475e-01  4.12003905e-01  3.70955825e-01  4.03643519e-01\n",
            "  4.41130430e-01 -9.77033198e-01  9.76212978e-01  2.04402223e-01\n",
            "  7.30958879e-01  1.18302532e-01  4.56178337e-01 -3.58906716e-01\n",
            " -1.82661712e+00  1.14189617e-01 -3.12755316e-01 -1.00277627e+00\n",
            " -1.56557322e+00  6.68430507e-01 -5.62978923e-01 -4.06885684e-01\n",
            "  8.29804316e-02  1.38392150e-01 -6.90688014e-01 -7.65070543e-02\n",
            "  1.19457245e+00 -7.66170561e-01 -2.51051843e-01 -1.49331295e+00\n",
            " -3.61757934e-01 -7.77776539e-02  4.27549452e-01 -6.78944662e-02\n",
            "  7.48548627e-01 -1.70640185e-01 -1.27812791e+00 -1.37854576e+00\n",
            "  1.46101579e-01  4.61373657e-01 -1.23564410e+00 -7.79936314e-01\n",
            "  8.38693500e-01  3.63027304e-01  2.73690606e-03 -1.53342474e+00\n",
            " -2.02657860e-02  8.30326319e-01  9.29510415e-01  2.61336148e-01\n",
            " -1.02450645e+00 -7.58553520e-02 -2.78167009e-01 -1.22152165e-01\n",
            " -1.76803112e+00 -9.45596024e-02 -1.87593508e+00 -7.42911696e-01\n",
            " -1.93098992e-01  1.11646213e-01  2.85774529e-01 -6.75242022e-02\n",
            " -3.34591419e-01  1.77341655e-01  4.54896033e-01 -4.55523372e-01\n",
            "  4.99860018e-01  8.48080039e-01 -2.30801091e-01  5.50851896e-02\n",
            "  3.64696205e-01  2.01426119e-01  5.09718657e-01  7.66543984e-01\n",
            "  1.06436861e+00 -5.74271083e-01 -7.64230788e-01  8.36789548e-01\n",
            "  8.65264773e-01  4.78995800e-01  3.08182817e-02 -3.13685805e-01\n",
            "  1.22950232e+00  1.80888578e-01  1.05126595e+00  2.61823595e-01\n",
            "  6.25478148e-01 -2.57961541e-01  1.48809910e+00  1.60108224e-01\n",
            "  7.93626249e-01 -8.59286845e-01 -7.52003253e-01 -1.08821332e+00\n",
            " -4.51654866e-02 -7.57845581e-01 -3.38013351e-01  1.10176265e+00\n",
            "  6.78450942e-01  5.12616396e-01 -1.27844866e-02 -7.41628036e-02\n",
            " -1.42623377e+00 -1.89869091e-01  7.91012764e-01  4.26329464e-01\n",
            " -7.20368207e-01  6.26080215e-01  5.13693094e-01  6.42679691e-01\n",
            "  6.93725944e-01 -1.42210412e+00 -7.73601055e-01 -6.84065938e-01\n",
            " -2.52786964e-01 -2.48845339e-01  1.31401226e-01 -5.02324462e-01\n",
            "  1.65544295e+00  2.49391887e-03 -9.10056829e-02 -3.99209648e-01\n",
            "  4.27120119e-01  1.39279276e-01 -2.06296057e-01 -2.54266292e-01\n",
            "  3.04580271e-01 -9.31832194e-02 -1.10844958e+00 -8.80649835e-02\n",
            " -2.44018644e-01 -6.33101642e-01  9.46687281e-01  6.62044525e-01\n",
            "  9.52786863e-01  8.42317283e-01 -8.88102531e-01 -3.18425819e-02\n",
            " -3.69531423e-01  8.41611505e-01 -3.94162178e-01 -3.88344198e-01\n",
            " -7.46209741e-01 -6.02211773e-01  1.36855388e+00  5.12582481e-01\n",
            " -1.30710411e+00 -1.12433828e-01  4.78569716e-02 -9.16845024e-01\n",
            "  1.92095172e+00 -2.10515618e+00  9.10364032e-01 -8.40061367e-01\n",
            " -4.56059754e-01  5.14895499e-01  3.35411310e-01 -7.66303465e-02\n",
            "  7.44095087e-01  1.15178234e-03 -1.77936152e-01 -5.77681482e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUw3_ird0Gb-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44ba7001-705f-4775-b6b1-5411b89f7c96"
      },
      "source": [
        "# An example on how to access a vector\n",
        "id_pemberhentian = corpus.word_field.vocab.stoi[\"pemberhentian\"]\n",
        "print(f\"Index for 'pemberhentian': {id_pemberhentian}\")\n",
        "vector_pemberhentian = corpus.word_field.vocab.vectors[id_pemberhentian]\n",
        "print(\n",
        "    \"Vector for 'pemberhentian' should be identical with the vector from the word2vec model: \" \n",
        "    + str(torch.equal(vector_pemberhentian, torch.as_tensor(corpus.wv_model.wv[\"pemberhentian\"])))\n",
        "    )"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index for 'pemberhentian': 8782\n",
            "Vector for 'pemberhentian' should be identical with the vector from the word2vec model: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "  import sys\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ph4mhgi1h-B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30b3ab1b-b506-43b7-8789-59bb86d0a2f4"
      },
      "source": [
        "# the initializer for Vocab provides special token handling ('<pad>', '<unk>')\n",
        "print(f\"index for pad token: {corpus.word_field.vocab.stoi[corpus.word_field.pad_token]}\")\n",
        "print(f\"index for unk token: {corpus.word_field.vocab.stoi[corpus.word_field.unk_token]}\")\n",
        "print(\"vector for index 0:\")\n",
        "print(corpus.word_field.vocab.vectors[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "index for pad token: 1\n",
            "index for unk token: 0\n",
            "vector for index 0:\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWH-_AF73Icr"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrE-USA215ma"
      },
      "source": [
        "class BiLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 embedding_dim,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 lstm_layers,\n",
        "                 emb_dropout,\n",
        "                 lstm_dropout,\n",
        "                 fc_dropout,\n",
        "                 word_pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        # LAYER 1: Embedding\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=input_dim,\n",
        "            embedding_dim=embedding_dim,\n",
        "            padding_idx=word_pad_idx\n",
        "        )\n",
        "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
        "        # LAYER 2: BiLSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=lstm_layers,\n",
        "            bidirectional=True,\n",
        "            dropout=lstm_dropout if lstm_layers > 1 else 0\n",
        "        )\n",
        "        # LAYER 3: Fully-connected\n",
        "        self.fc_dropout = nn.Dropout(fc_dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # times 2 for bidirectional\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        # sentence = [sentence length, batch size]\n",
        "        # embedding_out = [sentence length, batch size, embedding dim]\n",
        "        embedding_out = self.emb_dropout(self.embedding(sentence))\n",
        "        # lstm_out = [sentence length, batch size, hidden dim * 2]\n",
        "        lstm_out, _ = self.lstm(embedding_out)\n",
        "        # ner_out = [sentence length, batch size, output dim]\n",
        "        ner_out = self.fc(self.fc_dropout(lstm_out))\n",
        "        return ner_out\n",
        "\n",
        "    def init_weights(self):\n",
        "        # to initialize all parameters from normal distribution\n",
        "        # helps with converging during training\n",
        "        for name, param in self.named_parameters():\n",
        "            nn.init.normal_(param.data, mean=0, std=0.1)\n",
        "\n",
        "    ### BEGIN MODIFIED SECTION: WORD EMBEDDING ###\n",
        "    def init_embeddings(self, word_pad_idx, pretrained=None, freeze=True):\n",
        "        # initialize embedding for padding as zero\n",
        "        self.embedding.weight.data[word_pad_idx] = torch.zeros(self.embedding_dim)\n",
        "        if pretrained is not None:\n",
        "            # use built in function: from pretrained\n",
        "            # specify if embedding layer is trainable with the `freeze` param\n",
        "            self.embedding = nn.Embedding.from_pretrained(\n",
        "                embeddings=torch.as_tensor(pretrained),\n",
        "                padding_idx=word_pad_idx,\n",
        "                freeze=freeze\n",
        "            )\n",
        "    ### END MODIFIED SECTION ###\n",
        "\n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT693VHD3gpr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20247005-228f-4b48-fa02-a46787fae090"
      },
      "source": [
        "bilstm = BiLSTM(\n",
        "    input_dim=len(corpus.word_field.vocab),\n",
        "    embedding_dim=300,\n",
        "    hidden_dim=64,\n",
        "    output_dim=len(corpus.tag_field.vocab),\n",
        "    lstm_layers=2,\n",
        "    emb_dropout=0.5,\n",
        "    lstm_dropout=0.1,\n",
        "    fc_dropout=0.25,\n",
        "    word_pad_idx=corpus.word_pad_idx\n",
        ")\n",
        "bilstm.init_weights()\n",
        "### BEGIN MODIFIED SECTION: WORD EMBEDDING ###\n",
        "bilstm.init_embeddings(\n",
        "    word_pad_idx=corpus.word_pad_idx,\n",
        "    pretrained=corpus.word_field.vocab.vectors if corpus.wv_model else None,\n",
        "    freeze=True\n",
        ")\n",
        "### END MODIFIED SECTION ###\n",
        "print(f\"The model has {bilstm.count_parameters():,} trainable parameters.\")\n",
        "print(bilstm)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 289,558 trainable parameters.\n",
            "BiLSTM(\n",
            "  (embedding): Embedding(30050, 300, padding_idx=1)\n",
            "  (emb_dropout): Dropout(p=0.5, inplace=False)\n",
            "  (lstm): LSTM(300, 64, num_layers=2, dropout=0.1, bidirectional=True)\n",
            "  (fc_dropout): Dropout(p=0.25, inplace=False)\n",
            "  (fc): Linear(in_features=128, out_features=22, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCHTEl0V64JH"
      },
      "source": [
        "## Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5oYuxRT37FS"
      },
      "source": [
        "class Trainer(object):\n",
        "\n",
        "    def __init__(self, model, data, optimizer_cls, loss_fn_cls):\n",
        "        self.model = model\n",
        "        self.data = data\n",
        "        self.optimizer = optimizer_cls(model.parameters())\n",
        "        self.loss_fn = loss_fn_cls(ignore_index=self.data.tag_pad_idx)\n",
        "\n",
        "    @staticmethod\n",
        "    def epoch_time(start_time, end_time):\n",
        "        elapsed_time = end_time - start_time\n",
        "        elapsed_mins = int(elapsed_time / 60)\n",
        "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "        return elapsed_mins, elapsed_secs\n",
        "\n",
        "    def accuracy(self, preds, y):\n",
        "        max_preds = preds.argmax(dim=1, keepdim=True)  # get the index of the max probability\n",
        "        non_pad_elements = (y != self.data.tag_pad_idx).nonzero()  # prepare masking for paddings\n",
        "        correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
        "        return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]])\n",
        "\n",
        "    def epoch(self):\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "        self.model.train()\n",
        "        for batch in self.data.train_iter:\n",
        "            # text = [sent len, batch size]\n",
        "            text = batch.word\n",
        "            # tags = [sent len, batch size]\n",
        "            true_tags = batch.tag\n",
        "            self.optimizer.zero_grad()\n",
        "            pred_tags = self.model(text)\n",
        "            # to calculate the loss and accuracy, we flatten both prediction and true tags\n",
        "            # flatten pred_tags to [sent len, batch size, output dim]\n",
        "            pred_tags = pred_tags.view(-1, pred_tags.shape[-1])\n",
        "            # flatten true_tags to [sent len * batch size]\n",
        "            true_tags = true_tags.view(-1)\n",
        "            batch_loss = self.loss_fn(pred_tags, true_tags)\n",
        "            batch_acc = self.accuracy(pred_tags, true_tags)\n",
        "            batch_loss.backward()\n",
        "            self.optimizer.step()\n",
        "            epoch_loss += batch_loss.item()\n",
        "            epoch_acc += batch_acc.item()\n",
        "        return epoch_loss / len(self.data.train_iter), epoch_acc / len(self.data.train_iter)\n",
        "\n",
        "    def evaluate(self, iterator):\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            # similar to epoch() but model is in evaluation mode and no backprop\n",
        "            for batch in iterator:\n",
        "                text = batch.word\n",
        "                true_tags = batch.tag\n",
        "                pred_tags = self.model(text)\n",
        "                pred_tags = pred_tags.view(-1, pred_tags.shape[-1])\n",
        "                true_tags = true_tags.view(-1)\n",
        "                batch_loss = self.loss_fn(pred_tags, true_tags)\n",
        "                batch_acc = self.accuracy(pred_tags, true_tags)\n",
        "                epoch_loss += batch_loss.item()\n",
        "                epoch_acc += batch_acc.item()\n",
        "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "    def train(self, n_epochs):\n",
        "        for epoch in range(n_epochs):\n",
        "            start_time = time.time()\n",
        "            train_loss, train_acc = self.epoch()\n",
        "            end_time = time.time()\n",
        "            epoch_mins, epoch_secs = Trainer.epoch_time(start_time, end_time)\n",
        "            print(f\"Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
        "            print(f\"\\tTrn Loss: {train_loss:.3f} | Trn Acc: {train_acc * 100:.2f}%\")\n",
        "            val_loss, val_acc = self.evaluate(self.data.val_iter)\n",
        "            print(f\"\\tVal Loss: {val_loss:.3f} | Val Acc: {val_acc * 100:.2f}%\")\n",
        "        test_loss, test_acc = self.evaluate(self.data.test_iter)\n",
        "        print(f\"Test Loss: {test_loss:.3f} |  Test Acc: {test_acc * 100:.2f}%\")\n",
        "\n",
        "    def infer(self, sentence, true_tags=None):\n",
        "        self.model.eval()\n",
        "        # tokenize sentence\n",
        "        nlp = Indonesian()\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "        # transform to indices based on corpus vocab\n",
        "        numericalized_tokens = [self.data.word_field.vocab.stoi[t] for t in tokens]\n",
        "        # find unknown words\n",
        "        unk_idx = self.data.word_field.vocab.stoi[self.data.word_field.unk_token]\n",
        "        unks = [t for t, n in zip(tokens, numericalized_tokens) if n == unk_idx]\n",
        "        # begin prediction\n",
        "        token_tensor = torch.LongTensor(numericalized_tokens)\n",
        "        token_tensor = token_tensor.unsqueeze(-1)\n",
        "        predictions = self.model(token_tensor)\n",
        "        # convert results to tags\n",
        "        top_predictions = predictions.argmax(-1)\n",
        "        predicted_tags = [self.data.tag_field.vocab.itos[t.item()] for t in top_predictions]\n",
        "        # print inferred tags\n",
        "        max_len_token = max([len(token) for token in tokens] + [len(\"word\")])\n",
        "        max_len_tag = max([len(tag) for tag in predicted_tags] + [len(\"pred tag\")])\n",
        "        print(\n",
        "            f\"{'word'.ljust(max_len_token)}\\t{'unk'.ljust(max_len_token)}\\t{'pred tag'.ljust(max_len_tag)}\"\n",
        "            + (\"\\ttrue tag\" if true_tags else \"\")\n",
        "        )\n",
        "        for i, token in enumerate(tokens):\n",
        "            is_unk = \"✓\" if token in unks else \"\"\n",
        "            print(\n",
        "                f\"{token.ljust(max_len_token)}\\t{is_unk.ljust(max_len_token)}\\t{predicted_tags[i].ljust(max_len_tag)}\"\n",
        "                + (f\"\\t{true_tags[i]}\" if true_tags else \"\")\n",
        "            )\n",
        "        return tokens, predicted_tags, unks"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YYEXKVs7RfW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cc0e981-8477-464d-ff92-930f4858d7fe"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model=bilstm,\n",
        "    data=corpus,\n",
        "    optimizer_cls=Adam,\n",
        "    loss_fn_cls=nn.CrossEntropyLoss\n",
        ")\n",
        "trainer.train(15)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 10s\n",
            "\tTrn Loss: 1.289 | Trn Acc: 78.75%\n",
            "\tVal Loss: 0.683 | Val Acc: 85.67%\n",
            "Epoch: 02 | Epoch Time: 0m 10s\n",
            "\tTrn Loss: 0.758 | Trn Acc: 83.13%\n",
            "\tVal Loss: 0.543 | Val Acc: 86.33%\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrn Loss: 0.599 | Trn Acc: 84.67%\n",
            "\tVal Loss: 0.421 | Val Acc: 87.82%\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrn Loss: 0.485 | Trn Acc: 86.47%\n",
            "\tVal Loss: 0.359 | Val Acc: 88.80%\n",
            "Epoch: 05 | Epoch Time: 0m 10s\n",
            "\tTrn Loss: 0.422 | Trn Acc: 87.71%\n",
            "\tVal Loss: 0.324 | Val Acc: 89.44%\n",
            "Epoch: 06 | Epoch Time: 0m 10s\n",
            "\tTrn Loss: 0.383 | Trn Acc: 88.56%\n",
            "\tVal Loss: 0.305 | Val Acc: 89.84%\n",
            "Epoch: 07 | Epoch Time: 0m 10s\n",
            "\tTrn Loss: 0.352 | Trn Acc: 89.22%\n",
            "\tVal Loss: 0.285 | Val Acc: 90.53%\n",
            "Epoch: 08 | Epoch Time: 0m 10s\n",
            "\tTrn Loss: 0.328 | Trn Acc: 89.72%\n",
            "\tVal Loss: 0.274 | Val Acc: 91.00%\n",
            "Epoch: 09 | Epoch Time: 0m 10s\n",
            "\tTrn Loss: 0.307 | Trn Acc: 90.28%\n",
            "\tVal Loss: 0.276 | Val Acc: 90.86%\n",
            "Epoch: 10 | Epoch Time: 0m 10s\n",
            "\tTrn Loss: 0.293 | Trn Acc: 90.63%\n",
            "\tVal Loss: 0.264 | Val Acc: 91.07%\n",
            "Epoch: 11 | Epoch Time: 0m 10s\n",
            "\tTrn Loss: 0.280 | Trn Acc: 90.98%\n",
            "\tVal Loss: 0.267 | Val Acc: 91.26%\n",
            "Epoch: 12 | Epoch Time: 0m 10s\n",
            "\tTrn Loss: 0.267 | Trn Acc: 91.37%\n",
            "\tVal Loss: 0.272 | Val Acc: 90.97%\n",
            "Epoch: 13 | Epoch Time: 0m 9s\n",
            "\tTrn Loss: 0.262 | Trn Acc: 91.59%\n",
            "\tVal Loss: 0.269 | Val Acc: 90.89%\n",
            "Epoch: 14 | Epoch Time: 0m 10s\n",
            "\tTrn Loss: 0.254 | Trn Acc: 91.90%\n",
            "\tVal Loss: 0.266 | Val Acc: 91.45%\n",
            "Epoch: 15 | Epoch Time: 0m 10s\n",
            "\tTrn Loss: 0.242 | Trn Acc: 92.24%\n",
            "\tVal Loss: 0.266 | Val Acc: 90.95%\n",
            "Test Loss: 0.317 |  Test Acc: 89.66%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgkeD_Bv7axC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b80e8832-41ed-40b1-f516-5775fa1f7c85"
      },
      "source": [
        "# https://regional.kompas.com/read/2020/07/12/15554711/diduga-terlibat-perselingkuhan-ketua-kpu-sumba-barat-diberhentikan\n",
        "sentence = \"\\\"Menjatuhkan sanksi pemberhentian tetap kepada teradu Sophia Marlinda Djami selaku Ketua KPU Kabupaten Sumba Barat, sejak dibacakannya putusan ini\\\", ucap Alfitra dalam sidang putusan, Rabu (8/7/2020).\"\n",
        "tags = [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-PERSON\", \"I-PERSON\", \"L-PERSON\", \"O\", \"O\", \"B-ORGANIZATION\", \"I-ORGANIZATION\", \"I-ORGANIZATION\", \"L-ORGANIZATION\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"U-PERSON\", \"O\", \"O\", \"O\", \"O\", \"B-TIME\", \"I-TIME\", \"I-TIME\", \"I-TIME\", \"I-TIME\", \"I-TIME\", \"I-TIME\", \"L-TIME\", \"O\"]\n",
        "words, infer_tags, unknown_tokens = trainer.infer(sentence=sentence, true_tags=tags)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word         \tunk          \tpred tag      \ttrue tag\n",
            "\"            \t✓            \tU-PERSON      \tO\n",
            "menjatuhkan  \t             \tO             \tO\n",
            "sanksi       \t             \tO             \tO\n",
            "pemberhentian\t             \tO             \tO\n",
            "tetap        \t             \tO             \tO\n",
            "kepada       \t             \tO             \tO\n",
            "teradu       \t✓            \tO             \tO\n",
            "sophia       \t             \tB-PERSON      \tB-PERSON\n",
            "marlinda     \t✓            \tL-PERSON      \tI-PERSON\n",
            "djami        \t✓            \tO             \tL-PERSON\n",
            "selaku       \t             \tO             \tO\n",
            "ketua        \t             \tO             \tO\n",
            "kpu          \t             \tU-ORGANIZATION\tB-ORGANIZATION\n",
            "kabupaten    \t             \tO             \tI-ORGANIZATION\n",
            "sumba        \t             \tB-LOCATION    \tI-ORGANIZATION\n",
            "barat        \t             \tL-LOCATION    \tL-ORGANIZATION\n",
            ",            \t✓            \tO             \tO\n",
            "sejak        \t             \tO             \tO\n",
            "dibacakannya \t✓            \tO             \tO\n",
            "putusan      \t             \tO             \tO\n",
            "ini          \t             \tO             \tO\n",
            "\"            \t✓            \tO             \tO\n",
            ",            \t✓            \tO             \tO\n",
            "ucap         \t✓            \tO             \tO\n",
            "alfitra      \t✓            \tO             \tU-PERSON\n",
            "dalam        \t             \tO             \tO\n",
            "sidang       \t             \tO             \tO\n",
            "putusan      \t             \tO             \tO\n",
            ",            \t✓            \tO             \tO\n",
            "rabu         \t             \tB-TIME        \tB-TIME\n",
            "(            \t✓            \tI-TIME        \tI-TIME\n",
            "8            \t✓            \tI-TIME        \tI-TIME\n",
            "/            \t✓            \tI-TIME        \tI-TIME\n",
            "7            \t✓            \tI-TIME        \tI-TIME\n",
            "/            \t✓            \tI-TIME        \tI-TIME\n",
            "2020         \t✓            \tI-TIME        \tI-TIME\n",
            ")            \t✓            \tL-TIME        \tL-TIME\n",
            ".            \t             \tO             \tO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZV_LHnj7h6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "462e073c-0aa6-47db-d76d-02a5416cc182"
      },
      "source": [
        "# https://regional.kompas.com/read/2020/07/15/16583081/banjir-bandang-di-masamba-19-korban-meninggal-23-hilang-15000-mengungsi\n",
        "sentence = \"Sementara itu, Kepala Pelaksana BPBD Luwu Utara Muslim Muchtar mengatakan, terdapat 15.000 jiwa mengungsi akibat banjir bandang.\"\n",
        "tags = [\"O\", \"O\", \"O\", \"O\", \"O\", \"B-ORGANIZATION\", \"I-ORGANIZATION\", \"L-ORGANIZATION\", \"B-PERSON\", \"L-PERSON\", \"O\", \"O\", \"O\", \"U-QUANTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"]\n",
        "words, infer_tags, unknown_tokens = trainer.infer(sentence=sentence, true_tags=tags)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word      \tunk       \tpred tag      \ttrue tag\n",
            "sementara \t          \tO             \tO\n",
            "itu       \t          \tO             \tO\n",
            ",         \t✓         \tO             \tO\n",
            "kepala    \t          \tO             \tO\n",
            "pelaksana \t          \tO             \tO\n",
            "bpbd      \t✓         \tO             \tB-ORGANIZATION\n",
            "luwu      \t          \tB-LOCATION    \tI-ORGANIZATION\n",
            "utara     \t          \tL-LOCATION    \tL-ORGANIZATION\n",
            "muslim    \t          \tI-ORGANIZATION\tB-PERSON\n",
            "muchtar   \t          \tU-PERSON      \tL-PERSON\n",
            "mengatakan\t          \tO             \tO\n",
            ",         \t✓         \tO             \tO\n",
            "terdapat  \t          \tO             \tO\n",
            "15.000    \t✓         \tB-QUANTITY    \tU-QUANTITY\n",
            "jiwa      \t          \tI-QUANTITY    \tO\n",
            "mengungsi \t          \tL-QUANTITY    \tO\n",
            "akibat    \t          \tO             \tO\n",
            "banjir    \t          \tO             \tO\n",
            "bandang   \t          \tU-LOCATION    \tO\n",
            ".         \t          \tO             \tO\n"
          ]
        }
      ]
    }
  ]
}